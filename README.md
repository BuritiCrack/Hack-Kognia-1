# ‚öñÔ∏è Hack-Kognia RAG ‚Äî Asistente Legal Inteligente

Una aplicaci√≥n RAG (Retrieval-Augmented Generation) para responder consultas sobre documentos legales. Este proyecto fue desarrollado como demo para Hack Kognia 1.0 y permite cargar documentos (PDF, DOCX, TXT), indexarlos mediante embeddings y responder preguntas con contexto extra√≠do de los documentos.

## üë• Integrantes del Proyecto

- üë®‚Äçüíª Andr√©s Guti√©rrez
- üë©‚Äçüíª Manuela Cardona
- üë®‚Äçüíª Jos√© Buritica

## üöÄ Mejoras Recientes (v2.0)

### Sistema RAG Optimizado para Precisi√≥n y Relevancia

**Problema resuelto:** El sistema anterior devolv√≠a fragmentos con palabras similares pero no respond√≠a la pregunta real.

**Mejoras implementadas:**

1. **üìè Chunks m√°s grandes** (1000‚Üí1500 chars) con mejor overlap (200‚Üí300 chars)
2. **üîÑ Expansi√≥n de consultas** con sin√≥nimos autom√°ticos (ej: "requisitos"‚Üí"condiciones", "exigencias")
3. **üéØ Re-ranking por palabras clave** - prioriza fragmentos con t√©rminos relevantes de la pregunta
4. **üõ°Ô∏è Filtrado inteligente** - solo muestra fragmentos con 2+ palabras clave relevantes
5. **üìñ Preview contextual** - muestra contexto alrededor de las keywords encontradas
6. **üìä M√©tricas visibles** - similitud sem√°ntica + contador de palabras clave

**Resultado:** Respuestas hasta **3x m√°s relevantes** para preguntas espec√≠ficas.

üëâ **Ver detalles completos:** [OPTIMIZACION_RAG_AVANZADA.md](./OPTIMIZACION_RAG_AVANZADA.md)

---

## üôè Agradecimientos

Muchas gracias a la organizaci√≥n **TalentoTech y Kognia** por el apoyo y la confianza en nosotros durante el desarrollo de este proyecto.
**Estado**: Proyecto local / demo. No est√° desplegado en Render debido al uso de un modelo de HuggingFace que requiere almacenamiento grande (se llenaba el disco del servicio) y la cuenta gratuita no fue suficiente para las pruebas.

**Contenido r√°pido**

- **Backend**: FastAPI + LangChain + FAISS
- **Frontend**: HTML/CSS + JavaScript (vanilla)
- **Modelos/Embeddings**: HuggingFace (modelo local) / OpenAI embeddings seg√∫n configuraci√≥n

**Objetivo**: Demostrar un flujo RAG completo: carga de documentos ‚Üí chunking ‚Üí embeddings ‚Üí b√∫squeda sem√°ntica ‚Üí respuesta contextualizada.

**Directorio principal**

- `backend/` ‚Äî API y procesamiento RAG (Python, FastAPI)
- `frontend/` ‚Äî Interfaz web est√°tica

**¬øPor qu√© NO est√° en Render?**
Usamos un modelo de HuggingFace que almacena pesos grandes localmente. Al intentar desplegar en Render el almacenamiento se llen√≥ r√°pidamente y la capa gratuita no cubre el espacio/tiempo de c√≥mputo requerido. Por eso el servicio se mantiene para ejecuci√≥n local o en entornos con GPU/espacio suficiente.

**√çndice**

- **Qu√© hace**
- **Tecnolog√≠as**
- **Instalaci√≥n y ejecuci√≥n (local, Windows)**
- **Variables de entorno**
- **Endpoints principales**
- **Notas de despliegue y limitaciones**
- **Contribuir**
- **Licencia y autor√≠a**

**Qu√© se hizo (resumen)**

- Implementaci√≥n de backend en FastAPI que procesa documentos, crea embeddings, guarda vectores en FAISS y expone endpoints para upload, consulta, estado y reinicio.
- Frontend simple que permite subir archivos y consultar mediante un chat.
- Pipeline de extracci√≥n y chunking de documentos (PDF/DOCX/TXT).
- Integraci√≥n con LangChain para orquestar recuperaci√≥n + generaci√≥n.

**Caracter√≠sticas principales**

- Carga de m√∫ltiples documentos en una sola sesi√≥n
- Tokenizaci√≥n/segmentaci√≥n (chunking) configurable
- Indexado en FAISS (b√∫squeda sem√°ntica r√°pida)
- Respuestas con referencias a fragmentos fuente

**Tecnolog√≠as y librer√≠as**

- Lenguaje: Python 3.9+
- Framework: `FastAPI`
- Orquestador RAG: `langchain`
- Vector DB: `faiss` (o `faiss-cpu` seg√∫n instalaci√≥n)
- Procesamiento PDF: `pypdf`
- Procesamiento DOCX: `python-docx`
- Server: `uvicorn`
- Frontend: HTML5, CSS3, JavaScript (vanilla)

**Instalaci√≥n y ejecuci√≥n (Local ‚Äî Windows PowerShell)**

1. Clonar repositorio:

```powershell
git clone https://github.com/AndresFGutierrez/hack-kognia-rag-legal.git
cd hack-kognia-rag-legal
```

2. Crear y activar entorno virtual (Windows PowerShell):

```powershell
python -m venv venv
venv\Scripts\Activate
```

3. Instalar dependencias:

```powershell
pip install -r requirements.txt
```

4. Copiar archivo de ejemplo de variables de entorno y editar:

```powershell
copy .env.example .env
# Abrir .env y completar las variables (ver secci√≥n siguiente)
```

5. Ejecutar la API (modo desarrollo):

```powershell
# Opci√≥n 1: Ejecutar el script principal
python main.py

# Opci√≥n 2: Con uvicorn (recarga autom√°tica)
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

6. Abrir frontend en el navegador (si ejecutas frontend dev):

```powershell
cd frontend
# si hay un script npm para el frontend, usarlo; por ejemplo:
npm install
npm run dev
```

Luego abrir `http://localhost:8000` (o la url que indique el backend/frontend seg√∫n configuraci√≥n).

**Variables de entorno (ejemplo)**

- `OPENAI_API_KEY` ‚Äî (opcional) si usas OpenAI para embeddings o LLM.
- `HF_MODEL_PATH` ‚Äî ruta local al modelo HuggingFace (si usas modelo local).
- `OPENAI_MODEL` ‚Äî nombre del modelo OpenAI, si aplica.
- `CHUNK_SIZE` y `CHUNK_OVERLAP` ‚Äî par√°metros de chunking (si est√°n soportados por la app).

Edita `.env` seg√∫n tus necesidades.

**Endpoints principales**

- `GET /` ‚Äî Interfaz web principal
- `GET /api/health` ‚Äî Estado del servicio
- `POST /api/upload` ‚Äî Subir y procesar documentos (multipart/form-data)
- `POST /api/query` ‚Äî Preguntar al sistema: `{"question":"..."}`
- `POST /api/reset` ‚Äî Limpiar √≠ndice y documentos cargados
- `GET /api/status` ‚Äî Estado interno y n√∫mero de documentos cargados

Ejemplo de `POST /api/query` (JSON):

```json
{ "question": "¬øQu√© dice el documento sobre cl√°usulas de rescisi√≥n?" }
```

Respuesta esperada: objeto JSON con texto de respuesta, fragmentos fuente y nivel de confianza.

**Notas sobre despliegue y limitaciones**

- El proyecto funciona bien localmente o en servidores con espacio/CPU suficientes. No est√° desplegado en Render por la siguiente raz√≥n:
  - Usamos un modelo de `HuggingFace` con pesos grandes que se almacenan localmente. Durante intentos de despliegue en Render el almacenamiento se llen√≥ r√°pidamente y la capa gratuita no fue suficiente para realizar las pruebas. Por ello recomendamos desplegar en infra con disco persistente y/o instancias con GPU (AWS, GCP, Azure, o servidores propios).
- Para probar de forma econ√≥mica: usar modelos m√°s peque√±os, embeddings externos (OpenAI), o alojar los vectores en un servicio gestionado.

**Sugerencias para producci√≥n**

- Usar un servicio gestionado para vectores (Pinecone, Milvus Cloud, etc.) para evitar uso intensivo de disco.
- Separar almacenamiento de modelos y de la app (NFS, S3, o discos persistentes grandes).
- Limitar el tama√±o m√°ximo de archivos en uploads y controlar el uso de memoria.

**Contribuir**

- Fork del repo ‚Üí crear rama `feature/mi-mejora` ‚Üí PR con descripci√≥n clara y tests si aplica.

**Soporte y troubleshooting**

- Si la app no arranca, revisa:
  - `requirements.txt` instalado correctamente
  - Variables de entorno en `.env`
  - Disponibilidad de espacio en disco si usas modelo local HuggingFace
  - Logs en consola (FastAPI / Uvicorn)

**Autor**

- Proyecto desarrollado para Hackaton Kognia 1.0
